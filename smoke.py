# -*- coding: utf-8 -*-
"""smoke.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Wkhvi5MCYi3eDkzFGUW3C_ofPS427UJ
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pwd

!pip install pillow
from PIL import Image
import os
import matplotlib.pyplot as plt
import numpy as np
#to load photos without smoke
dir ='/content/gdrive/MyDrive/TensorFlow HomeWork/DontHave/' #
files = os.listdir(dir)
imgs = []
tags = []
for filename in files[:100]: #0, 1, 2, ..., 9
  imgs.append(Image.open(dir + filename))
  tags.append(0)
# to load photos with smoke
dir ='/content/gdrive/MyDrive/TensorFlow HomeWork/Have/'
files = os.listdir(dir)
imgs = []
tags = []
for filename in files[:100]: #0, 1, 2, ..., 9
  imgs.append(Image.open(dir + filename))
  tags.append(1)

print('tag:', tags[0])
plt.imshow(np.array(imgs[0]))
#plt.imshow(imgs[O])
print(np.array(imgs[0]).shape)

for i, img in enumerate(imgs):
  imgs[i] = img.resize((108, 72))
  plt.imshow(np.array(imgs[0]))

for i, img in enumerate(imgs):
  imgs[i] = img.convert('L')
plt.imshow(np.array(imgs[0]), cmap='hsv')

from sklearn.model_selection import train_test_split

for i, img in enumerate(imgs):
  imgs[i] = np.array(img)

training_imgs, testing_imgs, training_tags, testing_tags = train_test_split(np.array(imgs), np.array(tags), train_size=0.7)
print(training_tags)
print(testing_tags)

import tensorflow as tf
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(16, kernel_size=(5, 5), padding='same', input_shape=(72, 108, 1), activation='relu'),
  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(1280, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  #tf.keras.layers.Dense(10),
  #tf.keras.layers.Softmax()
  tf.keras.layers.Dense(2, activation='softmax')
])

print(training_imgs.shape)

training_imgs = training_imgs.reshape(training_imgs.shape[0], 72, 108, 1)
testing_imgs = testing_imgs.reshape(testing_imgs.shape[0], 72, 108, 1)

model.compile(
  optimizer='adam',
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy']
)

epoch_num = 100
history = model.fit(training_imgs, training_tags, validation_data=(testing_imgs, testing_tags), epochs=epoch_num)